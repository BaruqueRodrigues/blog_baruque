[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog_baruque",
    "section": "",
    "text": "Por que Tamanho Amostral Importa\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2024\n\n\nBaruque Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nRodando Multiplos Modelos - Tutorial\n\n\n\n\n\n\n\ntutorial\n\n\ncode\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2024\n\n\nBaruque Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nUsando o Reduce - Tutorial\n\n\n\n\n\n\n\ntutorial\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2024\n\n\nBaruque Rodrigues\n\n\n\n\n\n\n  \n\n\n\n\nBem Vindos\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\nBaruque Rodrigues\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Por que Tamanho Amostral Importa",
    "section": "",
    "text": "1. Introdução ao Poder Estatístico\nAfinal o que é Poder Estatístico? é a probabilidade de que o teste corretamente rejeite a hipótese nula quando ela é falsa, ou seja é a capacidade do teste de detectar um efeito real, se houver um. E pro que o poder estatístico serve? Para medirmos a eficácia com que um estudo pode fornecer respostas claras e confiáveis, evitando conclusões errôneas sobre os dados analisados.\n\nDefinindo Poder Estatístico\nAntes de definirmos o formalmente o que ele é precisamos entender 2 conceitos chave, Erro do tipo I e Erro do tipo II.\n\nErro Tipo I (α): É o chamado falso positivo, quando detectamos um efeito quando ele não existe. O Erro do Tipo I é cometido quando a nossa hipótese nula é incorretamente rejeitada, apesar de ser verdadeira.\nO nível de significância do teste é denotado por α, sendo a probabilidade de cometer um erro do Tipo I. Tipicamente, os valores de α são escolhidos como (p-valor de) 0.05 ou 0.01, indicando um limite de tolerância de 5% ou 1% para rejeitar incorretamente a hipótese nula.\nErro Tipo II (β): É conhecido como falso negativo, quando não encontramos o efeito apesar dele existir. Este erro acontece quando a hipótese nula deveria ser rejeitada, ou seja o efeito existe, mas não é detectado. O valor de β é afetado por vários fatores, incluindo o tamanho do efeito e o tamanho da amostra. Reduzir β (Erro do Tipo II) aumenta o poder do teste.\n\nUma forma de exemplificarmos quando podemos cometer o Erro do Tipo I e o Erro do Tipo II é imaginando o teste de covid. Digamos que você trabalha em uma repartição pública, onde o ar condicionado é constante, há acúmulo de mofo, as janelas nunca são abertas, e trabalha a menos de 1 metro dos seus colegas de trabalho. Na nossa situação hipotética, 3 membros do seu time estão em casa, pois estão com COVID. Você precisa fazer o teste, já que pode tanto contaminar outras pessoas, como também estar doente, vai para uma farmácia, compra o teste e está aguardando o resultado.\n\nSe você não tiver COVID, o Erro do Tipo I pode acontecer, já que pode indicar falsamente que você está infectado.\nSe você tiver COVID, o Erro do Tipo II pode acontecer, onde o teste indica de maneira incorreta que você não está infectado.\n\nAgora que já entendemos o que é o Erro do Tipo I (α) e o Erro do Tipo II (β), podemos definir formalmente o que é Poder Estatístico.\n\\[ Poder Estatístico = 1 - \\beta \\]\nO poder estatístico é definido como 1 menos a probabilidade de cometer um erro do Tipo II (β).\n\n\n\n2. Componentes do Poder Estatístico\nPara verificarmos a capacidade de um teste detectar um efeito quando ele existe, ou seja medir o poder estatístico, precisamos ter em mente alguns componentes, como o tamanho da amostra (n), o tamanho do efeito, o nível de significância (α) e a variabilidade dos dados, mas antes vamos definir o que são cada um desses elementos.\n\nTamanho da Amostra (n): são o número de casos utilizados no teste estatístico;\nTamanho do Efeito: a magnitude da diferença, ou a magnitude da relação investigada. Em outras palavras o grau de impacto de uma variável sobre a outra;\nNivel de Significância (α): a probabilidade de cometer o Erro Tipo I, comumente o valor crítico do p-valor;\nVariabilidade dos Dados: é a dispersão dos dados em torno de uma medida, geralmente é utilizada como desvio padrão;\n\n\n2.1 O Tamanho Amostral (n)\nO tamanho amostral é um dos fatores mais importantes no Poder Estatístico, já que por definição proporciona uma estimativa mais precisa da média populacional e reduz o erro padrão das estimativas.\nQuando estamos desenhando um estudo devemos fazer uma análise para identificarmos se a nossa amostra tem um tamanho adequado para atingir o poder estatístico necessário para identificarmos aquele efeito.\nPor exemplo, se quisermos identificar qual o Efeito dos Gastos de Campanha sobre a Obtenção Votos, primeiro temos que identificar qual o número mínimo de casos que a nossa amostra deve ter para que possamos identificar o efeito dos Gastos de Campanha sobre a Obtenção de Votos.\n\n\n2.2 Tamanho do Efeito\nJá o tamanho do Efeito afeta o poder estatístico já que efeitos maiores são mais fáceis de serem identificados que efeitos menores, mesmo que seja usada um mesmo n amostral.\nNo exemplo abaixo vamos construir 2 modelos com o mesmo tamanho amostral, e a mesma variabilidade dos dados (Média 50, Desvio Padrão = 100), mas vamos indicar um tamanho de efeito 10x maior no primeiro modelo,\nmau  set.seed(43)  #Tamanho Amostral n &lt;- 30  #Variável Independente x &lt;- rnorm(n)  #Variável dependente modelo 1 y1 &lt;- 10 * x + rnorm(n, mean = 50, sd = 100)  #Variável dependente modelo 1 y2 &lt;- 1 * x + rnorm(n,                     mean = 50, sd = 100)  modelo1 &lt;- lm(y1 ~ x) modelo2 &lt;- lm(y2 ~ x)  sjPlot::tab_model(modelo1, modelo2)}\nNo modelo a cima podemos enxergar como um efeito com magnitude 10 é detectado com um tamanho amostral de 30, enquanto um efeito de magnitude 1 não é detectado. Considerando que o efeito do modelo 2 é real, ele não seria detectado pelo poder estatístico do teste. Nesse caso cometeríamos o Erro do Tipo II onde o efeito existe mas não é detectado.\n\n\n2.3 Nível de Significância\nO nível de significância é um limiar usado para determinar quando rejeitar a hipótese nula em um teste estatístico, em outras palavras é a probabilidade de cometermos o Erro do Tipo I.\nSendo um dos componentes do poder estatístico que menos temos controle, já que é convencionado pela área. Em estudos de Ciência Política, Ciências Sociais, Sociologia e Economia é convencionado o uso de significância de 5%, ou seja um p valor de 0.05, que define o corte para decidir se um valor é ou não estatisticamente significante.\nVariar o nível de significância para baixo reduz a chance do Erro do Tipo I, todavia reduz o poder estatístico do teste, aumentando portanto o Erro do Tipo II, salvo se o tamanho amostral ou o tamanho do efeito sejam ajustados.\n\n\n2.4 Variabilidade dos Dados\nJá a variabilidade faz referência a dispersão dos valores em torno de uma medida, geralmente é o valor do desvio padrão em torno da média. O tamanho da variabilidade pode afetar a detecção de um efeito verdadeiro, já que em tamanhos amostrais menores valores influentes podem mover muito um valor médio.\nÉ sempre bom ter em mente que maior variabilidade implica em maior incerteza na sua estimativa, que acaba reduzindo seu Poder Estatístico. No exemplo abaixo aumentamos o Desvio Padrão, veja como apesar da média e o tamanho amostral continuarem constantes o nosso efeito deixa de ser detectado.\nmau  set.seed(43)  #Tamanho Amostral n &lt;- 30  #Variável Independente x &lt;- rnorm(n)  #Variável dependente modelo 1 y1 &lt;- 10 * x + rnorm(n, mean = 50, sd = 500)  #Variável dependente modelo 1 y2 &lt;- 1 * x + rnorm(n,                     mean = 50, sd = 500)   modelo1 &lt;- lm(y1 ~ x) modelo2 &lt;- lm(y2 ~ x)  sjPlot::tab_model(modelo1, modelo2)}\n\n\n\n3. Cálculo do Poder Estatístico\nDigamos que precisamos avaliar o impacto do programa Bolsa Família nas taxas de aprovação em alunos do ensino fundamental.\nUm estudo bem desenhado tem o cálculo do Poder Estatístico como etapa inicial, já que ele nos permitirá trabalhar com uma amostra que nos permita produzir resultados estatísticos robustos.\nOs parametros para o cálculo são:\n\nTamanho do Efeito: É geralmente estimado com base em estudos anteriores, para o nosso caso hipotético vamos sugerir que a participação no Bolsa Família aumenta as taxas de aprovação escolar em 10% comparado com alunos que não participam do programa.\nVariabilidade dos Dados: No nosso caso seria o Desvio Padrão na Taxa de Aprovação Escolar, vamos sugerir um desvio padrão de .20\nNível de Significância (α): Geralmente nas ciências sociais utilizamos o p valor de 0.05, indicando 5% de probabilidade de rejeitar erroneamente a hipótese nula se ela for verdadeira.\nPoder Desejado (1 - β): Vamos sugerir um poder de 80% ou 0.80, significando que há 80% de chance de detectar um efeito verdadeiro se ele realmente existir.\n\nNo nosso exemplo vamos comparar dois grupos: alunos que recebem o Bolsa Família versus alunos que não recebem. Vamos assumir que a aprovação no grupo de tratamento seja de .75 e a do grupo de controle seja .65, para calcularmos o tamanho da amostra\nmau # proporção de aprovação para o grupo Bolsa Família grupo_tratamento &lt;- 0.75  # proporção de aprovação para o grupo controle grupo_controle &lt;- 0.65   # calculo do tamanho do efeito  tamanho_efeito &lt;- pwr::ES.h(grupo_tratamento, grupo_controle)    # Calcula o tamanho da amostra necessário pwr::pwr.p.test(h = tamanho_efeito, sig.level = 0.05, power = 0.80, alternative = \"two.sided\")}\nNesse caso o Tamanho Amostral para Detectarmos o Efeito Caso ele exista é de 163"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Quem sou eu",
    "section": "",
    "text": "Eu sou Baruque, não vou me credenciar por que eu acho isso cringe, todavia sou cientista social e cientista político (aposentado) de formação. Trabalho com Ciência de Dados e R desde 2019, passei tanto pelo setor público como também pela iniciativa privada, hoje tô na Quaest.\nAqui eu vou falar um pouco sobre estatística e ciência de dados. A principio esse blog surge como uma forma de estrururar alguns posts que eu fazia no twitter, mas a ideia aqui é dar um pouco mais corpo a eles, e ajudar a servir a comunidade."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Bem Vindos",
    "section": "",
    "text": "Fala, galera. Tudo certinho?\n\nEsse é meu primeiro post aqui no blog, basicamente pra fazer o lançamento dele, todavia não vou gastar energia nele, quem sabe um dia eu volte aqui e refaça esse post."
  },
  {
    "objectID": "posts/tutorial-reduce/index.html",
    "href": "posts/tutorial-reduce/index.html",
    "title": "Usando o Reduce - Tutorial",
    "section": "",
    "text": "Em R, as funções reduce e map são utilizadas para manipular coleções de dados, como listas ou vetores, mas cada uma tem propósitos distintos e é adequada para diferentes tipos de operações. A ideia aqui é apresentar como utilizar essas o reduce para ganhar tempo e produtividade nas nossas rotinas de dados.\n\n\n\nA função reduce é parte do pacote {purrr} que faz parte do {tidyverse} e é usada para reduzir uma coleção de valores a um único valor. A operação de redução é realizada aplicando uma função que toma dois argumentos de entrada de cada vez, acumulando o resultado conforme percorre a coleção.\n\nNo exemplo abaixo vamos somar uma coleção de valores, existem algumas formas de fazer isso. A mais intuitiva é inserir os valores e o operador de soma.\n\n1+2+3+4+5\n\n[1] 15\n\n\nUsando o purrr::reduce eu posso fazer essa operação de forma mais simples, no primeiro argumento indico o vetor de elementos que receberá a função, e no segundo a função que será executada acumulando o resultado enquanto percorre o vetor.\n\n1+2+3+4+5\n\n[1] 15\n\n\nQuando usar:\n\nQuando você precisa combinar elementos de uma lista ou vetor em um único resultado com base em uma operação específica.\nOperações comuns incluem a soma de números, a multiplicação de elementos ou a combinação de estruturas de dados mais complexas de uma maneira específica.\n\n\n\n\nImagine que você está trabalhando com dados de vendas mensais de várias lojas e quer calcular o total de vendas anuais combinadas de todas as lojas. Aqui, cada coluna são vendas mensais de uma loja, e você quer somar todas as vendas de todos os meses de todas as lojas.\n\ntibble_vendas &lt;- dplyr::tibble(\n  mes = 1:6,\n  vendas_loja_1 = c(12000, 13000, 11000, 14000, 15000, 14500),\n  vendas_loja_2 = c(11500, 12000, 13000, 13500, 14000, 12500),\n  vendas_loja_3 = c(13000, 13500, 14000, 14500, 15000, 15500))\n\nExistem multiplas formas de executar essa tarefa, todavia por critérios pedagógicos, vamos fazer utilizando o {tidyverse}. Na primeira abordagem vamos fazer a soma sem utilizar o reduce e na segunda vamos utilizá-lo, assim podemos comparar os resultados e o quão difícil é implementá-lo.\n\ntibble_vendas %&gt;% \n  mutate(\n    # Sem o reduce só executamos a soma indicando as colunas\n    total_vendas = vendas_loja_1 + vendas_loja_2 + vendas_loja_3,\n    # No reduce precisamos indicar que queremos remover a coluna mes\n    # para que ela não seja inserida no calculo\n    total_vendas_via_reduce = reduce(tibble_vendas %&gt;%\n                                     select(-mes), `+`)\n         )\n\n# A tibble: 6 × 6\n    mes vendas_loja_1 vendas_loja_2 vendas_loja_3 total_vendas\n  &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1     1         12000         11500         13000        36500\n2     2         13000         12000         13500        38500\n3     3         11000         13000         14000        38000\n4     4         14000         13500         14500        42000\n5     5         15000         14000         15000        44000\n6     6         14500         12500         15500        42500\n# ℹ 1 more variable: total_vendas_via_reduce &lt;dbl&gt;\n\n\nSem o reduce só executamos a soma indicando as colunas já no reduce precisamos indicar que queremos remover a coluna mes para que ela não seja inserida no cálculo. Em uma primeira vista fazer uso do reduce, ou aprender ele não faz sentido já que é mais viável inserir os calculos na mão, correto? Vendo o exemplo acima, tenho que concordar com você, é melhor indicar manualmente as colunas do que aprender o uso de uma nova função.\nPreparei o Exemplo abaixo pra tentar te fazer mudar de ideia\nNo dataset com muitas lojas as vendas de 6 meses para 365 lojas, nesse caso calculas o total de vendas sem utilizar o reduce é humanamente impossível.\n\ndataset_com_muitas_lojas \n\n# A tibble: 6 × 366\n    mes vendas_loja_1 vendas_loja_2 vendas_loja_3 vendas_loja_4 vendas_loja_5\n  &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1     1        11448.        13791.        14130.        13106.        12512.\n2     2        11338.        11365.        14568.        10443.        13462.\n3     3        12036.        11410.        14560.        11128.        13037.\n4     4        11113.        14415.        11333.        13028.        11249.\n5     5        11814.        12721.        13109.        13898.        10909.\n6     6        13351.        11659.        12262.        13144.        13488.\n# ℹ 360 more variables: vendas_loja_6 &lt;dbl&gt;, vendas_loja_7 &lt;dbl&gt;,\n#   vendas_loja_8 &lt;dbl&gt;, vendas_loja_9 &lt;dbl&gt;, vendas_loja_10 &lt;dbl&gt;,\n#   vendas_loja_11 &lt;dbl&gt;, vendas_loja_12 &lt;dbl&gt;, vendas_loja_13 &lt;dbl&gt;,\n#   vendas_loja_14 &lt;dbl&gt;, vendas_loja_15 &lt;dbl&gt;, vendas_loja_16 &lt;dbl&gt;,\n#   vendas_loja_17 &lt;dbl&gt;, vendas_loja_18 &lt;dbl&gt;, vendas_loja_19 &lt;dbl&gt;,\n#   vendas_loja_20 &lt;dbl&gt;, vendas_loja_21 &lt;dbl&gt;, vendas_loja_22 &lt;dbl&gt;,\n#   vendas_loja_23 &lt;dbl&gt;, vendas_loja_24 &lt;dbl&gt;, vendas_loja_25 &lt;dbl&gt;, …\n\n\nPor mais que aprender funções novas seja complicado, por te fazer gastar tempo que você poderia estar alocando em resolver problemas, ele se paga em pouquíssimo tempo, já que o ganho que produtividade vai ser cumulativo pro resto da sua vida.\n\ndataset_com_muitas_lojas %&gt;% \n  reframe(\n   # total_vendas = vendas_loja_1 + vendas_loja_2 + vendas_loja_3,\n    total_vendas_via_reduce = reduce(dataset_com_muitas_lojas %&gt;%\n                                     select(-mes), `+`)\n         )\n\n# A tibble: 6 × 1\n  total_vendas_via_reduce\n                    &lt;dbl&gt;\n1                4581447.\n2                4559736.\n3                4579527.\n4                4523289.\n5                4545922.\n6                4547279."
  },
  {
    "objectID": "posts/tutorial-reduce/index.html#tópicos-sobre-programação-funcional",
    "href": "posts/tutorial-reduce/index.html#tópicos-sobre-programação-funcional",
    "title": "Usando o Reduce - Tutorial",
    "section": "",
    "text": "Em R, as funções reduce e map são utilizadas para manipular coleções de dados, como listas ou vetores, mas cada uma tem propósitos distintos e é adequada para diferentes tipos de operações. A ideia aqui é apresentar como utilizar essas o reduce para ganhar tempo e produtividade nas nossas rotinas de dados.\n\n\n\nA função reduce é parte do pacote {purrr} que faz parte do {tidyverse} e é usada para reduzir uma coleção de valores a um único valor. A operação de redução é realizada aplicando uma função que toma dois argumentos de entrada de cada vez, acumulando o resultado conforme percorre a coleção.\n\nNo exemplo abaixo vamos somar uma coleção de valores, existem algumas formas de fazer isso. A mais intuitiva é inserir os valores e o operador de soma.\n\n1+2+3+4+5\n\n[1] 15\n\n\nUsando o purrr::reduce eu posso fazer essa operação de forma mais simples, no primeiro argumento indico o vetor de elementos que receberá a função, e no segundo a função que será executada acumulando o resultado enquanto percorre o vetor.\n\n1+2+3+4+5\n\n[1] 15\n\n\nQuando usar:\n\nQuando você precisa combinar elementos de uma lista ou vetor em um único resultado com base em uma operação específica.\nOperações comuns incluem a soma de números, a multiplicação de elementos ou a combinação de estruturas de dados mais complexas de uma maneira específica.\n\n\n\n\nImagine que você está trabalhando com dados de vendas mensais de várias lojas e quer calcular o total de vendas anuais combinadas de todas as lojas. Aqui, cada coluna são vendas mensais de uma loja, e você quer somar todas as vendas de todos os meses de todas as lojas.\n\ntibble_vendas &lt;- dplyr::tibble(\n  mes = 1:6,\n  vendas_loja_1 = c(12000, 13000, 11000, 14000, 15000, 14500),\n  vendas_loja_2 = c(11500, 12000, 13000, 13500, 14000, 12500),\n  vendas_loja_3 = c(13000, 13500, 14000, 14500, 15000, 15500))\n\nExistem multiplas formas de executar essa tarefa, todavia por critérios pedagógicos, vamos fazer utilizando o {tidyverse}. Na primeira abordagem vamos fazer a soma sem utilizar o reduce e na segunda vamos utilizá-lo, assim podemos comparar os resultados e o quão difícil é implementá-lo.\n\ntibble_vendas %&gt;% \n  mutate(\n    # Sem o reduce só executamos a soma indicando as colunas\n    total_vendas = vendas_loja_1 + vendas_loja_2 + vendas_loja_3,\n    # No reduce precisamos indicar que queremos remover a coluna mes\n    # para que ela não seja inserida no calculo\n    total_vendas_via_reduce = reduce(tibble_vendas %&gt;%\n                                     select(-mes), `+`)\n         )\n\n# A tibble: 6 × 6\n    mes vendas_loja_1 vendas_loja_2 vendas_loja_3 total_vendas\n  &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1     1         12000         11500         13000        36500\n2     2         13000         12000         13500        38500\n3     3         11000         13000         14000        38000\n4     4         14000         13500         14500        42000\n5     5         15000         14000         15000        44000\n6     6         14500         12500         15500        42500\n# ℹ 1 more variable: total_vendas_via_reduce &lt;dbl&gt;\n\n\nSem o reduce só executamos a soma indicando as colunas já no reduce precisamos indicar que queremos remover a coluna mes para que ela não seja inserida no cálculo. Em uma primeira vista fazer uso do reduce, ou aprender ele não faz sentido já que é mais viável inserir os calculos na mão, correto? Vendo o exemplo acima, tenho que concordar com você, é melhor indicar manualmente as colunas do que aprender o uso de uma nova função.\nPreparei o Exemplo abaixo pra tentar te fazer mudar de ideia\nNo dataset com muitas lojas as vendas de 6 meses para 365 lojas, nesse caso calculas o total de vendas sem utilizar o reduce é humanamente impossível.\n\ndataset_com_muitas_lojas \n\n# A tibble: 6 × 366\n    mes vendas_loja_1 vendas_loja_2 vendas_loja_3 vendas_loja_4 vendas_loja_5\n  &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1     1        11448.        13791.        14130.        13106.        12512.\n2     2        11338.        11365.        14568.        10443.        13462.\n3     3        12036.        11410.        14560.        11128.        13037.\n4     4        11113.        14415.        11333.        13028.        11249.\n5     5        11814.        12721.        13109.        13898.        10909.\n6     6        13351.        11659.        12262.        13144.        13488.\n# ℹ 360 more variables: vendas_loja_6 &lt;dbl&gt;, vendas_loja_7 &lt;dbl&gt;,\n#   vendas_loja_8 &lt;dbl&gt;, vendas_loja_9 &lt;dbl&gt;, vendas_loja_10 &lt;dbl&gt;,\n#   vendas_loja_11 &lt;dbl&gt;, vendas_loja_12 &lt;dbl&gt;, vendas_loja_13 &lt;dbl&gt;,\n#   vendas_loja_14 &lt;dbl&gt;, vendas_loja_15 &lt;dbl&gt;, vendas_loja_16 &lt;dbl&gt;,\n#   vendas_loja_17 &lt;dbl&gt;, vendas_loja_18 &lt;dbl&gt;, vendas_loja_19 &lt;dbl&gt;,\n#   vendas_loja_20 &lt;dbl&gt;, vendas_loja_21 &lt;dbl&gt;, vendas_loja_22 &lt;dbl&gt;,\n#   vendas_loja_23 &lt;dbl&gt;, vendas_loja_24 &lt;dbl&gt;, vendas_loja_25 &lt;dbl&gt;, …\n\n\nPor mais que aprender funções novas seja complicado, por te fazer gastar tempo que você poderia estar alocando em resolver problemas, ele se paga em pouquíssimo tempo, já que o ganho que produtividade vai ser cumulativo pro resto da sua vida.\n\ndataset_com_muitas_lojas %&gt;% \n  reframe(\n   # total_vendas = vendas_loja_1 + vendas_loja_2 + vendas_loja_3,\n    total_vendas_via_reduce = reduce(dataset_com_muitas_lojas %&gt;%\n                                     select(-mes), `+`)\n         )\n\n# A tibble: 6 × 1\n  total_vendas_via_reduce\n                    &lt;dbl&gt;\n1                4581447.\n2                4559736.\n3                4579527.\n4                4523289.\n5                4545922.\n6                4547279."
  },
  {
    "objectID": "posts/tutorial-multiplos-modelos/index.html",
    "href": "posts/tutorial-multiplos-modelos/index.html",
    "title": "Rodando Multiplos Modelos - Tutorial",
    "section": "",
    "text": "Um processo muito comum na prática de análise de dados é encontrar um modelo que melhor resolva o seu problema, onde temos uma relação que queremos entender, por exemplo o número de funcionários de um Posto de Saúde da Família afeta a mortalidade de uma faixa etária dos municípios. O processo as vezes acaba sendo a construção de multiplos modelos, e a verificação um a um dos coeficientes. Meu objetivo aqui é apresentar uma forma de fazer com que esse processo repetitivo e manual possa ser acelerado, de maneira que o tempo do analista possa ser concentrado no que realmente importa, na compreensão do modelo.\n\n\nPara aqueles que já foram meus alunos, ou trabalharam comigo sabem que esse é o um acronimo que eu sempre repito, afinal o O conceito DRY (Don’t Repeat Yourself) é um princípio fundamental na programação, pois enfatiza a redução da redundância.\nEm essência, DRY sugere que qualquer pedaço de código deve ter uma única e clara chamada. Em vez de duplicar código, o princípio DRY incentiva a reutilização de componentes existentes, melhorando a manutenção e a legibilidade do código.\nTrocando em míudos, tente programar fazendo com que cada chunk de código não seja repetida.\n\n\n\nEsse tutorial vai assumir que você já entenda os conceitos de pivoteamento e de pipeamento de funções, caso você não domine esses conteúdos, recomendo que você visite o r4ds.\n\n\n\nVamos utilizar dados de painel do DATASUS, armazenados em um arquivo RDS. Nosso objetivo é:\nCalcular um modelo de regressão para cada uma das taxas de mortalidade, onde cada taxa será a nossa variável dependente (VD).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndados_painel &lt;- rio::import(\"dados/dados_painel.rds\")\n\nglimpse(dados_painel)\n\nRows: 81,525\nColumns: 22\n$ municipio              &lt;chr&gt; \"110001 Alta Floresta D'Oeste\", \"110001 Alta Fl…\n$ ano                    &lt;chr&gt; \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\",…\n$ pessoas_psf            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 5, 5140, 7531, 15105, 17608, …\n$ pop_menor_1            &lt;dbl&gt; 557, 543, 515, 522, 529, 535, 541, 556, 563, 55…\n$ pop_1a4                &lt;dbl&gt; 2664, 2601, 2353, 2387, 2415, 2444, 2473, 2540,…\n$ pop_15a59              &lt;dbl&gt; 14627, 14281, 15820, 16049, 16240, 16435, 16629…\n$ pop_maior_60           &lt;dbl&gt; 1157, 1130, 1486, 1508, 1526, 1544, 1561, 1603,…\n$ pop_mun_todos          &lt;dbl&gt; 25630, 25023, 26533, 26916, 27237, 27563, 27886…\n$ mort_menor_1           &lt;dbl&gt; 17, 36, 10, 12, 12, 7, 5, 12, 8, 4, 8, 4, 10, 6…\n$ mort_1a4               &lt;dbl&gt; 1, 3, 2, 0, 3, 3, 1, 1, 2, 0, 3, 2, 2, 1, 0, 0,…\n$ mort_15a59             &lt;dbl&gt; 33, 64, 30, 45, 46, 44, 26, 37, 33, 24, 50, 44,…\n$ mort_maior_60          &lt;dbl&gt; 37, 53, 50, 52, 45, 60, 51, 53, 75, 34, 75, 59,…\n$ mort_todos             &lt;dbl&gt; 90, 167, 97, 111, 107, 116, 84, 105, 119, 64, 1…\n$ tx_mort_menor_1        &lt;dbl&gt; 30.520646, 66.298343, 19.417476, 22.988506, 22.…\n$ tx_mort_menor_1a4      &lt;dbl&gt; 0.3753754, 1.1534025, 0.8499788, 0.0000000, 1.2…\n$ tx_mort_menor_15a59    &lt;dbl&gt; 225.6102, 448.1479, 189.6334, 280.3913, 283.251…\n$ tx_mort_menor_maior_60 &lt;dbl&gt; 3197.926, 4690.265, 3364.738, 3448.276, 2948.88…\n$ tx_mort_todos          &lt;dbl&gt; 351.1510, 667.3860, 365.5825, 412.3941, 392.848…\n$ tx_cobertura_psf       &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000,…\n$ cod_uf                 &lt;chr&gt; \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"11\",…\n$ existe_psf             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ sigla_uf               &lt;chr&gt; \"RO\", \"RO\", \"RO\", \"RO\", \"RO\", \"RO\", \"RO\", \"RO\",…\n\n\nObserve aqui que temos variáveis que indicam taxas de mortalidade para diferentes grupos etários, já que a taxa de mortalidade é nossa variável dependente uma das soluções seria executar um modelo para cada uma delas.\nO problema dessa abordagem é que o número de modelos escalaria muito rápido. Se temos 5 taxas de mortalidade serão 5 modelos, se adicionamos 1 variável de controle a cada uma delas, seriam 10 modelos.\n\nplm::plm(tx_mort_menor_1 ~ pessoas_psf,\n         data = dados_painel,\n         model = \"within\",\n         index = c(\"ano\", \"municipio\")) %&gt;% \n  summary()\n\nOneway (individual) effect Within Model\n\nCall:\nplm::plm(formula = tx_mort_menor_1 ~ pessoas_psf, data = dados_painel, \n    model = \"within\", index = c(\"ano\", \"municipio\"))\n\nUnbalanced Panel: n = 15, T = 5372-5427, N = 81218\n\nResiduals:\n    Min.  1st Qu.   Median  3rd Qu.     Max. \n-17.1364 -11.0568  -1.2128   6.6585 242.8223 \n\nCoefficients:\n              Estimate Std. Error t-value Pr(&gt;|t|)\npessoas_psf 6.6285e-08 4.6189e-08  1.4351   0.1513\n\nTotal Sum of Squares:    13746000\nResidual Sum of Squares: 13746000\nR-Squared:      2.5361e-05\nAdj. R-Squared: -0.00015936\nF-statistic: 2.05942 on 1 and 81202 DF, p-value: 0.15127\n\n\nJá uma outra solução é utilizando tópicos de programação funcional. Primeiro nós iremos mudar o formato dos dados, de maneira que existam 5 variáveis, pessoas_psf, que é nossa Variável Independente, nomes_mortalidade que contém os nomes das nossas variáveis dependentes e values_mortalidade que contém os valores dessas variáveis, e as variáveis municipio, e ano\n\ndados_painel %&gt;% \n  select(contains(\"tx_mor\"), pessoas_psf, municipio, ano) %&gt;%\n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) \n\n# A tibble: 407,625 × 5\n   pessoas_psf municipio              ano   nomes_mortalidade values_mortalidade\n         &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;\n 1           0 110001 Alta Floresta … 1998  tx_mort_menor_1               30.5  \n 2           0 110001 Alta Floresta … 1998  tx_mort_menor_1a4              0.375\n 3           0 110001 Alta Floresta … 1998  tx_mort_menor_15…            226.   \n 4           0 110001 Alta Floresta … 1998  tx_mort_menor_ma…           3198.   \n 5           0 110001 Alta Floresta … 1998  tx_mort_todos                351.   \n 6           0 110001 Alta Floresta … 1999  tx_mort_menor_1               66.3  \n 7           0 110001 Alta Floresta … 1999  tx_mort_menor_1a4              1.15 \n 8           0 110001 Alta Floresta … 1999  tx_mort_menor_15…            448.   \n 9           0 110001 Alta Floresta … 1999  tx_mort_menor_ma…           4690.   \n10           0 110001 Alta Floresta … 1999  tx_mort_todos                667.   \n# ℹ 407,615 more rows\n\n\nAgora nós iremos transformar nossos dados em nested_data. Onde iremos criar um subconjunto de dados para cada uma das taxas de mortalidade. Observe que a coluna data contém um dataset com 2 colunas e 81 mil linhas, nesse dataset está contida a variável pessoas_psf e a variável values_mortalidade que contém o o número das taxas de mortalidade por faixa\n\ndados_painel %&gt;% \n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) %&gt;% \n  nest(data = -nomes_mortalidade) \n\n# A tibble: 5 × 2\n  nomes_mortalidade      data                  \n  &lt;chr&gt;                  &lt;list&gt;                \n1 tx_mort_menor_1        &lt;tibble [81,525 × 18]&gt;\n2 tx_mort_menor_1a4      &lt;tibble [81,525 × 18]&gt;\n3 tx_mort_menor_15a59    &lt;tibble [81,525 × 18]&gt;\n4 tx_mort_menor_maior_60 &lt;tibble [81,525 × 18]&gt;\n5 tx_mort_todos          &lt;tibble [81,525 × 18]&gt;\n\n\nAgora podemos executar o nosso modelo, para tal iremos criar uma coluna no dataset chamada modelo_plm, que conterá um modelo para cada uma das nossas variáveis.\n\ndados_painel %&gt;% \n  select(contains(\"tx_mor\"), pessoas_psf, ano, municipio) %&gt;%\n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) %&gt;% \n  nest(data = -nomes_mortalidade) %&gt;% \n   mutate(\n    # Executando o modelo para cada variável\n    modelo_plm = map(data, ~plm::plm(values_mortalidade ~ pessoas_psf,\n                                      data = .,\n                                      model = \"within\",\n                                      index = c(\"ano\", \"municipio\"))),\n    # Transformando o modelo em um formato tabular\n    modelo_plm = map(modelo_plm, broom::tidy)\n  )\n\n# A tibble: 5 × 3\n  nomes_mortalidade      data                  modelo_plm      \n  &lt;chr&gt;                  &lt;list&gt;                &lt;list&gt;          \n1 tx_mort_menor_1        &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n2 tx_mort_menor_1a4      &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n3 tx_mort_menor_15a59    &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n4 tx_mort_menor_maior_60 &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n5 tx_mort_todos          &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n\n\nAgora só precisamos remover a coluna data, que não tem nenhum valor analítico, e desenlistar a variável modelo_plm para checarmos os modelo.\n\ndados_painel %&gt;% \n  select(contains(\"tx_mor\"), pessoas_psf, ano, municipio) %&gt;%\n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) %&gt;% \n  nest(data = -nomes_mortalidade) %&gt;% \n   mutate(\n    # Executando o modelo para cada variável\n    modelo_plm = map(data, ~plm::plm(values_mortalidade ~ pessoas_psf,\n                                      data = .,\n                                      model = \"within\",\n                                      index = c(\"ano\", \"municipio\"))),\n    # Transformando o modelo em um formato tabular\n    modelo_plm = map(modelo_plm, broom::tidy)\n  ) %&gt;% \n  select(-data) %&gt;% \n  unnest_wider(modelo_plm)\n\n# A tibble: 5 × 6\n  nomes_mortalidade      term             estimate   std.error statistic p.value\n  &lt;chr&gt;                  &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 tx_mort_menor_1        pessoas_psf 0.0000000663      4.62e-8      1.44 0.151  \n2 tx_mort_menor_1a4      pessoas_psf 0.00000000414     3.18e-9      1.30 0.193  \n3 tx_mort_menor_15a59    pessoas_psf 0.00000113        4.71e-7      2.40 0.0163 \n4 tx_mort_menor_maior_60 pessoas_psf 0.0000141         5.11e-6      2.75 0.00591\n5 tx_mort_todos          pessoas_psf 0.00000133        8.63e-7      1.54 0.124"
  },
  {
    "objectID": "posts/tutorial-multiplos-modelos/index.html#section",
    "href": "posts/tutorial-multiplos-modelos/index.html#section",
    "title": "Rodando Multiplos Modelos - Tutorial",
    "section": "",
    "text": "Um processo muito comum na prática de análise de dados é encontrar um modelo que melhor resolva o seu problema, onde temos uma relação que queremos entender, por exemplo o número de funcionários de um Posto de Saúde da Família afeta a mortalidade de uma faixa etária dos municípios. O processo as vezes acaba sendo a construção de multiplos modelos, e a verificação um a um dos coeficientes. Meu objetivo aqui é apresentar uma forma de fazer com que esse processo repetitivo e manual possa ser acelerado, de maneira que o tempo do analista possa ser concentrado no que realmente importa, na compreensão do modelo.\n\n\nPara aqueles que já foram meus alunos, ou trabalharam comigo sabem que esse é o um acronimo que eu sempre repito, afinal o O conceito DRY (Don’t Repeat Yourself) é um princípio fundamental na programação, pois enfatiza a redução da redundância.\nEm essência, DRY sugere que qualquer pedaço de código deve ter uma única e clara chamada. Em vez de duplicar código, o princípio DRY incentiva a reutilização de componentes existentes, melhorando a manutenção e a legibilidade do código.\nTrocando em míudos, tente programar fazendo com que cada chunk de código não seja repetida.\n\n\n\nEsse tutorial vai assumir que você já entenda os conceitos de pivoteamento e de pipeamento de funções, caso você não domine esses conteúdos, recomendo que você visite o r4ds.\n\n\n\nVamos utilizar dados de painel do DATASUS, armazenados em um arquivo RDS. Nosso objetivo é:\nCalcular um modelo de regressão para cada uma das taxas de mortalidade, onde cada taxa será a nossa variável dependente (VD).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndados_painel &lt;- rio::import(\"dados/dados_painel.rds\")\n\nglimpse(dados_painel)\n\nRows: 81,525\nColumns: 22\n$ municipio              &lt;chr&gt; \"110001 Alta Floresta D'Oeste\", \"110001 Alta Fl…\n$ ano                    &lt;chr&gt; \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\",…\n$ pessoas_psf            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 5, 5140, 7531, 15105, 17608, …\n$ pop_menor_1            &lt;dbl&gt; 557, 543, 515, 522, 529, 535, 541, 556, 563, 55…\n$ pop_1a4                &lt;dbl&gt; 2664, 2601, 2353, 2387, 2415, 2444, 2473, 2540,…\n$ pop_15a59              &lt;dbl&gt; 14627, 14281, 15820, 16049, 16240, 16435, 16629…\n$ pop_maior_60           &lt;dbl&gt; 1157, 1130, 1486, 1508, 1526, 1544, 1561, 1603,…\n$ pop_mun_todos          &lt;dbl&gt; 25630, 25023, 26533, 26916, 27237, 27563, 27886…\n$ mort_menor_1           &lt;dbl&gt; 17, 36, 10, 12, 12, 7, 5, 12, 8, 4, 8, 4, 10, 6…\n$ mort_1a4               &lt;dbl&gt; 1, 3, 2, 0, 3, 3, 1, 1, 2, 0, 3, 2, 2, 1, 0, 0,…\n$ mort_15a59             &lt;dbl&gt; 33, 64, 30, 45, 46, 44, 26, 37, 33, 24, 50, 44,…\n$ mort_maior_60          &lt;dbl&gt; 37, 53, 50, 52, 45, 60, 51, 53, 75, 34, 75, 59,…\n$ mort_todos             &lt;dbl&gt; 90, 167, 97, 111, 107, 116, 84, 105, 119, 64, 1…\n$ tx_mort_menor_1        &lt;dbl&gt; 30.520646, 66.298343, 19.417476, 22.988506, 22.…\n$ tx_mort_menor_1a4      &lt;dbl&gt; 0.3753754, 1.1534025, 0.8499788, 0.0000000, 1.2…\n$ tx_mort_menor_15a59    &lt;dbl&gt; 225.6102, 448.1479, 189.6334, 280.3913, 283.251…\n$ tx_mort_menor_maior_60 &lt;dbl&gt; 3197.926, 4690.265, 3364.738, 3448.276, 2948.88…\n$ tx_mort_todos          &lt;dbl&gt; 351.1510, 667.3860, 365.5825, 412.3941, 392.848…\n$ tx_cobertura_psf       &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000,…\n$ cod_uf                 &lt;chr&gt; \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"11\",…\n$ existe_psf             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ sigla_uf               &lt;chr&gt; \"RO\", \"RO\", \"RO\", \"RO\", \"RO\", \"RO\", \"RO\", \"RO\",…\n\n\nObserve aqui que temos variáveis que indicam taxas de mortalidade para diferentes grupos etários, já que a taxa de mortalidade é nossa variável dependente uma das soluções seria executar um modelo para cada uma delas.\nO problema dessa abordagem é que o número de modelos escalaria muito rápido. Se temos 5 taxas de mortalidade serão 5 modelos, se adicionamos 1 variável de controle a cada uma delas, seriam 10 modelos.\n\nplm::plm(tx_mort_menor_1 ~ pessoas_psf,\n         data = dados_painel,\n         model = \"within\",\n         index = c(\"ano\", \"municipio\")) %&gt;% \n  summary()\n\nOneway (individual) effect Within Model\n\nCall:\nplm::plm(formula = tx_mort_menor_1 ~ pessoas_psf, data = dados_painel, \n    model = \"within\", index = c(\"ano\", \"municipio\"))\n\nUnbalanced Panel: n = 15, T = 5372-5427, N = 81218\n\nResiduals:\n    Min.  1st Qu.   Median  3rd Qu.     Max. \n-17.1364 -11.0568  -1.2128   6.6585 242.8223 \n\nCoefficients:\n              Estimate Std. Error t-value Pr(&gt;|t|)\npessoas_psf 6.6285e-08 4.6189e-08  1.4351   0.1513\n\nTotal Sum of Squares:    13746000\nResidual Sum of Squares: 13746000\nR-Squared:      2.5361e-05\nAdj. R-Squared: -0.00015936\nF-statistic: 2.05942 on 1 and 81202 DF, p-value: 0.15127\n\n\nJá uma outra solução é utilizando tópicos de programação funcional. Primeiro nós iremos mudar o formato dos dados, de maneira que existam 5 variáveis, pessoas_psf, que é nossa Variável Independente, nomes_mortalidade que contém os nomes das nossas variáveis dependentes e values_mortalidade que contém os valores dessas variáveis, e as variáveis municipio, e ano\n\ndados_painel %&gt;% \n  select(contains(\"tx_mor\"), pessoas_psf, municipio, ano) %&gt;%\n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) \n\n# A tibble: 407,625 × 5\n   pessoas_psf municipio              ano   nomes_mortalidade values_mortalidade\n         &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;\n 1           0 110001 Alta Floresta … 1998  tx_mort_menor_1               30.5  \n 2           0 110001 Alta Floresta … 1998  tx_mort_menor_1a4              0.375\n 3           0 110001 Alta Floresta … 1998  tx_mort_menor_15…            226.   \n 4           0 110001 Alta Floresta … 1998  tx_mort_menor_ma…           3198.   \n 5           0 110001 Alta Floresta … 1998  tx_mort_todos                351.   \n 6           0 110001 Alta Floresta … 1999  tx_mort_menor_1               66.3  \n 7           0 110001 Alta Floresta … 1999  tx_mort_menor_1a4              1.15 \n 8           0 110001 Alta Floresta … 1999  tx_mort_menor_15…            448.   \n 9           0 110001 Alta Floresta … 1999  tx_mort_menor_ma…           4690.   \n10           0 110001 Alta Floresta … 1999  tx_mort_todos                667.   \n# ℹ 407,615 more rows\n\n\nAgora nós iremos transformar nossos dados em nested_data. Onde iremos criar um subconjunto de dados para cada uma das taxas de mortalidade. Observe que a coluna data contém um dataset com 2 colunas e 81 mil linhas, nesse dataset está contida a variável pessoas_psf e a variável values_mortalidade que contém o o número das taxas de mortalidade por faixa\n\ndados_painel %&gt;% \n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) %&gt;% \n  nest(data = -nomes_mortalidade) \n\n# A tibble: 5 × 2\n  nomes_mortalidade      data                  \n  &lt;chr&gt;                  &lt;list&gt;                \n1 tx_mort_menor_1        &lt;tibble [81,525 × 18]&gt;\n2 tx_mort_menor_1a4      &lt;tibble [81,525 × 18]&gt;\n3 tx_mort_menor_15a59    &lt;tibble [81,525 × 18]&gt;\n4 tx_mort_menor_maior_60 &lt;tibble [81,525 × 18]&gt;\n5 tx_mort_todos          &lt;tibble [81,525 × 18]&gt;\n\n\nAgora podemos executar o nosso modelo, para tal iremos criar uma coluna no dataset chamada modelo_plm, que conterá um modelo para cada uma das nossas variáveis.\n\ndados_painel %&gt;% \n  select(contains(\"tx_mor\"), pessoas_psf, ano, municipio) %&gt;%\n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) %&gt;% \n  nest(data = -nomes_mortalidade) %&gt;% \n   mutate(\n    # Executando o modelo para cada variável\n    modelo_plm = map(data, ~plm::plm(values_mortalidade ~ pessoas_psf,\n                                      data = .,\n                                      model = \"within\",\n                                      index = c(\"ano\", \"municipio\"))),\n    # Transformando o modelo em um formato tabular\n    modelo_plm = map(modelo_plm, broom::tidy)\n  )\n\n# A tibble: 5 × 3\n  nomes_mortalidade      data                  modelo_plm      \n  &lt;chr&gt;                  &lt;list&gt;                &lt;list&gt;          \n1 tx_mort_menor_1        &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n2 tx_mort_menor_1a4      &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n3 tx_mort_menor_15a59    &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n4 tx_mort_menor_maior_60 &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n5 tx_mort_todos          &lt;tibble [81,525 × 4]&gt; &lt;tibble [1 × 5]&gt;\n\n\nAgora só precisamos remover a coluna data, que não tem nenhum valor analítico, e desenlistar a variável modelo_plm para checarmos os modelo.\n\ndados_painel %&gt;% \n  select(contains(\"tx_mor\"), pessoas_psf, ano, municipio) %&gt;%\n  # Organizando os dados em formato longo, empilhando as variáveis de mortalidade\n  pivot_longer(\n    cols = c(tx_mort_menor_1:tx_mort_todos),\n    names_to = \"nomes_mortalidade\",\n    values_to = \"values_mortalidade\"\n  ) %&gt;% \n  nest(data = -nomes_mortalidade) %&gt;% \n   mutate(\n    # Executando o modelo para cada variável\n    modelo_plm = map(data, ~plm::plm(values_mortalidade ~ pessoas_psf,\n                                      data = .,\n                                      model = \"within\",\n                                      index = c(\"ano\", \"municipio\"))),\n    # Transformando o modelo em um formato tabular\n    modelo_plm = map(modelo_plm, broom::tidy)\n  ) %&gt;% \n  select(-data) %&gt;% \n  unnest_wider(modelo_plm)\n\n# A tibble: 5 × 6\n  nomes_mortalidade      term             estimate   std.error statistic p.value\n  &lt;chr&gt;                  &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 tx_mort_menor_1        pessoas_psf 0.0000000663      4.62e-8      1.44 0.151  \n2 tx_mort_menor_1a4      pessoas_psf 0.00000000414     3.18e-9      1.30 0.193  \n3 tx_mort_menor_15a59    pessoas_psf 0.00000113        4.71e-7      2.40 0.0163 \n4 tx_mort_menor_maior_60 pessoas_psf 0.0000141         5.11e-6      2.75 0.00591\n5 tx_mort_todos          pessoas_psf 0.00000133        8.63e-7      1.54 0.124"
  }
]